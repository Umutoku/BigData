{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4cc5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e21d23",
   "metadata": {},
   "source": [
    "# SparkContext oluşturma 1: SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f80f7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f307c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark = SparkSession.builder \\\n",
    ".master(\"local[4]\") \\\n",
    ".appName(\"Rdd-Olusturmak\") \\\n",
    ".config(\"spark.executor.memory\",\"4g\") \\\n",
    ".config(\"spark.driver.memory\",\"2g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a066f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95964507",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccba1e5",
   "metadata": {},
   "source": [
    "# SparkContext oluşturma 2: ParkSession ve SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74c3bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    ".setMaster(\"local[4]\") \\\n",
    ".setAppName(\"RDD-Olusturmaa\") \\\n",
    ".setExecutorEnv(\"spark.executor.memory\",\"4g\") \\\n",
    ".setExecutorEnv(\"spark.driver.memory\",\"4g\") \\\n",
    "\n",
    "pyspark = SparkSession.builder \\\n",
    ".config(conf=conf) \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e7a2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f812fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d6118",
   "metadata": {},
   "source": [
    "# SparkContext oluşturma 3: SparkContext ve SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7dd3c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = SparkConf() \\\n",
    ".setMaster(\"local\") \\\n",
    ".setAppName(\"RDD-Olusturma\") \\\n",
    ".setExecutorEnv(\"spark.executor.memory\",\"4g\") \\\n",
    ".setExecutorEnv(\"spark.driver.memory\",\"4g\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83faa7f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:750)\r\n\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2456)\r\n\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2452)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2452)\r\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2541)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:84)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-8e0a50cf08ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m--> 118\u001b[1;33m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[1;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \"\"\"\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1523\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1524\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1525\u001b[1;33m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[0;32m   1526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1527\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:750)\r\n\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2456)\r\n\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2452)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2452)\r\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2541)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:84)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf = sparkConf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bdaca4",
   "metadata": {},
   "source": [
    "# Python tuple Rdd oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc0dc53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([('Ahmet',25),('Cemal',29),('İnci',38),('Burcu',33)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d15e8d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ahmet', 25), ('Cemal', 29), ('İnci', 38), ('Burcu', 33)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30c0b63",
   "metadata": {},
   "source": [
    "# Python listelerinden Rdd oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ef678c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.parallelize([['Ahmet',25],['Cemal',29],['İnci',38],['Burcu',33]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "68786e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ahmet', 25], ['Cemal', 29], ['İnci', 38]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "098678e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4ddf891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b8fbff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sayilarRDD = sc.parallelize([[1,2,3],[4,5,6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "38f99424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sayilarRDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b20e45",
   "metadata": {},
   "source": [
    "# Python sözlükten (dictionary) RDD oluşturmak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9781e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sözlük Oluşturma\n",
    "my_dict ={\n",
    "    \"Ogrenci\":['Ali','Mehmet','Ayse'],\n",
    "    \"Notlar\":[70,80,90]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4748d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52be8a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDF = pd.DataFrame(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7f29cd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ogrenci</th>\n",
       "      <th>Notlar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ali</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mehmet</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ayse</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ogrenci  Notlar\n",
       "0     Ali      70\n",
       "1  Mehmet      80\n",
       "2    Ayse      90"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28ba3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_from_pandasDF = pyspark.createDataFrame(pdDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "237be792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|Ogrenci|Notlar|\n",
      "+-------+------+\n",
      "|    Ali|    70|\n",
      "| Mehmet|    80|\n",
      "|   Ayse|    90|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_from_pandasDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ab0ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_from_pandas = rdd_from_pandasDF.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1ec2f55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Ogrenci='Ali', Notlar=70),\n",
       " Row(Ogrenci='Mehmet', Notlar=80),\n",
       " Row(Ogrenci='Ayse', Notlar=90)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_from_pandas.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8ea7b",
   "metadata": {},
   "source": [
    "# Metin dosyalarından RDD oluşturmak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "755f7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_metin = sc.textFile(\"C:\\\\Users\\\\umuto\\\\OneDrive\\\\Masaüstü\\\\OnlineRetail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bddf6de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo;StockCode;Description;Quantity;InvoiceDate;UnitPrice;CustomerID;Country',\n",
       " '536365;85123A;WHITE HANGING HEART T-LIGHT HOLDER;6;1.12.2010 08:26;2,55;17850;United Kingdom',\n",
       " '536365;71053;WHITE METAL LANTERN;6;1.12.2010 08:26;3,39;17850;United Kingdom',\n",
       " '536365;84406B;CREAM CUPID HEARTS COAT HANGER;8;1.12.2010 08:26;2,75;17850;United Kingdom',\n",
       " '536365;84029G;KNITTED UNION FLAG HOT WATER BOTTLE;6;1.12.2010 08:26;3,39;17850;United Kingdom',\n",
       " '536365;84029E;RED WOOLLY HOTTIE WHITE HEART.;6;1.12.2010 08:26;3,39;17850;United Kingdom',\n",
       " '536365;22752;SET 7 BABUSHKA NESTING BOXES;2;1.12.2010 08:26;7,65;17850;United Kingdom',\n",
       " '536365;21730;GLASS STAR FROSTED T-LIGHT HOLDER;6;1.12.2010 08:26;4,25;17850;United Kingdom',\n",
       " '536366;22633;HAND WARMER UNION JACK;6;1.12.2010 08:28;1,85;17850;United Kingdom',\n",
       " '536366;22632;HAND WARMER RED POLKA DOT;6;1.12.2010 08:28;1,85;17850;United Kingdom']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_metin.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680850b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
